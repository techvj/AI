{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverEvent(Event):\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "class RerankEvent(Event):\n",
    "\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import  CompactAndRefine\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-ollama \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.llms.ollama import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.legacy.readers.file.base import SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 22:52:20.349338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-08 22:52:21.164541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ctx:Context, ev:StartEvent) -> StopEvent | None:\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embed_model\n",
    "        )\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx:Context, ev:StartEvent) -> RetrieverEvent | None:\n",
    "\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "        \n",
    "        print(f\"query the database with: {query}\")\n",
    "\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        if index is None:\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=2)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def rerank(self, ctx:Context, ev:RetrieverEvent) -> RerankEvent:\n",
    "\n",
    "        # ranker = LLMRerank(\n",
    "        #     choice_batch_size=5, top_n=3, llm=Ollama(model=\"llama3:latest\", request_timeout=120.0)\n",
    "        # )\n",
    "        \n",
    "        ranker = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "\n",
    "        print(await ctx.get(\"query\", default=None),flush=True)\n",
    "        new_nodes = ranker.postprocess_nodes(\n",
    "            ev.nodes, query_str=await ctx.get(\"query\", default=None)\n",
    "        )\n",
    "        print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "        return RerankEvent(nodes=new_nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx:Context, ev:RerankEvent) -> StopEvent:\n",
    "        llm = Ollama(model=\"llama3:latest\", request_timeout=120.0)\n",
    "        summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall llama-index --yes\n",
    "# !pip install llama-index --upgrade --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RAGWorkflow()\n",
    "\n",
    "# Ingest the documents\n",
    "index = await w.run(dirname=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query the database with: How was Llama2 trained?\n",
      "Retrieved 2 nodes\n",
      "How was Llama2 trained?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked nodes to 2\n",
      "The training of Llama 2 began with the pretraining of Llama 2 using publicly available online sources. This was followed by the creation of an initial version of Llama 2-Chat through the application of supervised fine-tuning. The model was then iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO)."
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "result = await w.run(query=\"How was Llama2 trained?\", index=index)\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-08 22:23:02--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
      "--2025-03-08 22:23:18--  http://arxiv.org/pdf/2307.09288\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  2.93MB/s    in 4.4s    \n",
      "\n",
      "2025-03-08 22:23:22 (2.99 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
